{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import validators\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "allrecipe_base_url = 'https://www.allrecipes.com'\n",
    "\n",
    "def scrape_page(URL):\n",
    "    '''\n",
    "    Function to extract all the information on the webpage using BeautifulSoup.\n",
    "    '''\n",
    "    page = requests.get(URL, verify=False)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def lemmatize(string):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    words = string.split(' ')\n",
    "    for word in words :\n",
    "        lem_word = lemmatizer.lemmatize(word)\n",
    "        lemmatized.append(lem_word)\n",
    "        \n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def get_allergen_diet(fourth_soup):\n",
    "    \n",
    "    vegan = False\n",
    "    vegetarian = False\n",
    "    egg_free = True\n",
    "    dairy_free = True\n",
    "    nut_free = True\n",
    "    shellfish_free = True\n",
    "    \n",
    "    dairy = ['butter', 'cheese', 'milk', 'cream']\n",
    "    nuts = ['almond', 'brazil nut', 'walnut', 'hazelnut', 'cashew', 'pine nut', 'macadamia nut', 'pistachio']\n",
    "    shellfish = ['shrimp', 'crab', 'lobster', 'clam', 'oyster', 'mussel', 'prawn']\n",
    "    \n",
    "    recipe_ingredients = [x.find('input').get('data-ingredient') for x in fourth_soup.find_all('li', class_='ingredients-item')]\n",
    "                \n",
    "    for ing in recipe_ingredients:\n",
    "        ing = lemmatize(ing)\n",
    "\n",
    "        if 'egg' in ing:\n",
    "            egg_free = False\n",
    "        if any(x in ing for x in dairy):\n",
    "            dairy_free = False\n",
    "        if any(x in ing for x in nuts):\n",
    "            nut_free = False\n",
    "        if any(x in ing for x in shellfish):\n",
    "            shellfish_free = False\n",
    "\n",
    "    if 'vegan' in recipe_title:\n",
    "        vegan = True\n",
    "    if 'vegetarian' in recipe_title:\n",
    "        vegetarian = True\n",
    "    \n",
    "    return vegan, vegetarian, egg_free, dairy_free, nut_free, shellfish_free\n",
    "\n",
    "def get_rating(fourth_soup):\n",
    "    rating_review = fourth_soup.find('div', {'aria-label': 'Ratings and Reviews'})\n",
    "    recipe_rating = rating_review.get('data-ratings-average')\n",
    "    \n",
    "    if recipe_rating == '':\n",
    "        recipe_rating = 'None'\n",
    "        recipe_rating_count = '0'\n",
    "        recipe_review_count = '0'\n",
    "    else:\n",
    "        recipe_rating_count = rating_review.get('data-ratings-count')\n",
    "        recipe_review_count = rating_review.get('data-reviews-count')\n",
    "        if recipe_rating_count == '':\n",
    "            recipe_rating_count = '0'\n",
    "            \n",
    "        if recipe_review_count == '':\n",
    "            recipe_rating_count = '0'\n",
    "    \n",
    "    return recipe_rating, recipe_rating_count, recipe_review_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "soup = scrape_page('https://dish.allrecipes.com/faq-sitemap/')\n",
    "main_ingredients_links = [x.get('href') for x in soup.find_all('p')[1].find_all('a')]\n",
    "main_ingredients = [x.text.replace('Recipes', '').lstrip().rstrip() for x in soup.find_all('p')[1].find_all('a')]\n",
    "\n",
    "if os.path.exists('allrecipe-data.csv'):\n",
    "    scraped_df = pd.read_csv('data.csv', index_col=[0])\n",
    "    \n",
    "    main_cat_list = scraped_df['Main Category'].tolist()\n",
    "    sub_cat_list = scraped_df['Subcategory'].tolist()\n",
    "    title_list = scraped_df['Title'].tolist()\n",
    "    rating_list = scraped_df['Rating'].tolist()\n",
    "    rating_count_list = scraped_df['Rating Count'].tolist()\n",
    "    review_count_list = scraped_df['Review Count'].tolist()\n",
    "    image_url_list = scraped_df['Image URL'].tolist()\n",
    "    recipe_url_list = scraped_df['Recipe URL'].tolist()\n",
    "    nutrition_list = scraped_df['Nutrition'].tolist()\n",
    "    vegan_list = scraped_df['Vegan'].tolist()\n",
    "    vegetarian_list = scraped_df['Vegetarian'].tolist()\n",
    "    egg_free_list = scraped_df['Egg Free'].tolist()\n",
    "    dairy_free_list = scraped_df['Dairy Free'].tolist()\n",
    "    nut_free_list = scraped_df['Nut Free'].tolist()\n",
    "    shellfish_free_list = scraped_df['Shellfish Free'].tolist()\n",
    "else:\n",
    "    main_cat_list = []\n",
    "    sub_cat_list = []\n",
    "    title_list = []\n",
    "    rating_list = []\n",
    "    rating_count_list = []\n",
    "    review_count_list = []\n",
    "    image_url_list = []\n",
    "    recipe_url_list = []\n",
    "    nutrition_list = []\n",
    "    vegan_list = []\n",
    "    vegetarian_list = []\n",
    "    egg_free_list = []\n",
    "    dairy_free_list = []\n",
    "    nut_free_list = []\n",
    "    shellfish_free_list = []\n",
    "\n",
    "for link, ingredient in zip(main_ingredients_links, main_ingredients):\n",
    "    print('Scraping {} Recipes ...'.format(ingredient))\n",
    "    \n",
    "    second_soup = scrape_page(link)\n",
    "    sub_cat_results = second_soup.find_all('li', class_='carouselNav__listItem recipeCarousel__listItem')\n",
    "    \n",
    "    for sub_cat_result in sub_cat_results:\n",
    "        sub_cat_title = sub_cat_result.find('a').get('data-tracking-content-headline').lower()\n",
    "        if os.path.exists('data.csv'):\n",
    "            if sub_cat_title in scraped_df['Subcategory'].unique():\n",
    "                continue\n",
    "            \n",
    "        sub_cat_link = sub_cat_result.find('a').get('href')\n",
    "        base_link = sub_cat_link\n",
    "        \n",
    "        recipe_count = 0\n",
    "        page_num = 1\n",
    "        print('Scraping {} Recipes ...'.format(sub_cat_title))\n",
    "        while recipe_count < 150:\n",
    "            third_soup = scrape_page(sub_cat_link)\n",
    "            if page_num == 1:\n",
    "                results = third_soup.find_all('div', class_='component card card__category')\n",
    "            else:\n",
    "                results = third_soup.find_all('div', class_='component tout')\n",
    "            \n",
    "            if (page_num == 1 and len(results) < 36) or (page_num > 2 and len(results) < 24 and recipe_count + len(results) < 150):\n",
    "                break\n",
    "    \n",
    "            for result in tqdm(results):\n",
    "                # Extract recipe title\n",
    "                recipe_title = result.a['title'].lower()\n",
    "                \n",
    "                # Extract recipe image URL\n",
    "                recipe_image = result.find('noscript').find('img').get('src')\n",
    "                \n",
    "                # Extract recipe URL\n",
    "                if page_num == 1:\n",
    "                    recipe_url = result.a['href']\n",
    "                else:\n",
    "                    recipe_url = allrecipe_base_url + result.a['href']\n",
    "                    \n",
    "                # Check if URL is valid\n",
    "                if not validators.url(recipe_url):\n",
    "                    continue\n",
    "                \n",
    "                # Scrape recipe page\n",
    "                fourth_soup = scrape_page(recipe_url)\n",
    "                \n",
    "                # Extract recipe rating and number of ratings\n",
    "                # All recipes should have a rating (unrated is a rating)\n",
    "                # If no rating is found on page, this means that the page does not contain a specific recipe\n",
    "                try:\n",
    "                    recipe_rating, recipe_rating_count, recipe_review_count = get_rating(fourth_soup)\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "                \n",
    "                # Extract nutritional info\n",
    "                try:\n",
    "                    recipe_nutrition = fourth_soup.find('div', class_='partial recipe-nutrition-section').find('div', class_='section-body').text.replace('Full Nutrition', '').lstrip().rstrip()\n",
    "                except AttributeError:\n",
    "                    recipe_nutrition = 'None'\n",
    "                \n",
    "                # Extract possible allergens/dietary restrictions\n",
    "                vegan, vegetarian, egg_free, dairy_free, nut_free, shellfish_free = get_allergen_diet(fourth_soup)\n",
    "\n",
    "                main_cat_list.append(ingredient.lower())\n",
    "                sub_cat_list.append(sub_cat_title)\n",
    "                title_list.append(recipe_title)\n",
    "                rating_list.append(recipe_rating)\n",
    "                rating_count_list.append(recipe_rating_count)\n",
    "                review_count_list.append(recipe_review_count)\n",
    "                image_url_list.append(recipe_image)\n",
    "                recipe_url_list.append(recipe_url)\n",
    "                nutrition_list.append(recipe_nutrition)\n",
    "                egg_free_list.append(egg_free)\n",
    "                dairy_free_list.append(dairy_free)\n",
    "                nut_free_list.append(nut_free)\n",
    "                shellfish_free_list.append(shellfish_free)\n",
    "                vegan_list.append(vegan)\n",
    "                vegetarian_list.append(vegetarian)\n",
    "                recipe_count += 1\n",
    "\n",
    "            page_num += 1\n",
    "            sub_cat_link = base_link + '?page={}'.format(page_num)\n",
    "            \n",
    "        df = pd.DataFrame(list(zip(main_cat_list, sub_cat_list, recipe_url_list, title_list, image_url_list, rating_list, rating_count_list, review_count_list, nutrition_list, vegan_list, vegetarian_list, egg_free_list, dairy_free_list, nut_free_list, shellfish_free_list)), \n",
    "         columns=['Main Category', 'Subcategory', 'Recipe URL', 'Title', 'Image URL', 'Rating', 'Rating Count', 'Review Count', 'Nutrition', 'Vegan', 'Vegetarian', 'Egg Free', 'Dairy Free', 'Nut Free', 'Shellfish Free'])\n",
    "        \n",
    "        df.to_csv('allrecipe-data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main cats and sub cats"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "scraped_csv = pd.read_csv(\"allrecipe-data.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "scraped_csv.drop(\"Unnamed: 0\", axis = 1, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "scraped_csv = scraped_csv.sample(frac=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "scraped_csv.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2458 entries, 4464 to 10612\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Main Category   2458 non-null   object\n",
      " 1   Subcategory     2458 non-null   object\n",
      " 2   Recipe URL      2458 non-null   object\n",
      " 3   Title           2458 non-null   object\n",
      " 4   Image URL       2458 non-null   object\n",
      " 5   Rating          2458 non-null   object\n",
      " 6   Rating Count    2458 non-null   int64 \n",
      " 7   Review Count    2458 non-null   int64 \n",
      " 8   Nutrition       2458 non-null   object\n",
      " 9   Vegan           2458 non-null   bool  \n",
      " 10  Vegetarian      2458 non-null   bool  \n",
      " 11  Egg Free        2458 non-null   bool  \n",
      " 12  Dairy Free      2458 non-null   bool  \n",
      " 13  Nut Free        2458 non-null   bool  \n",
      " 14  Shellfish Free  2458 non-null   bool  \n",
      "dtypes: bool(6), int64(2), object(7)\n",
      "memory usage: 206.4+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# import json\n",
    "import pickle\n",
    "maincats = scraped_csv[\"Main Category\"].unique()\n",
    "maincat2id = {}\n",
    "id2subcat2id = {}\n",
    "for i, k in enumerate(maincats):\n",
    "    maincat2id[k]=i\n",
    "    print(k, len(subcats))\n",
    "    subcats = scraped_csv[scraped_csv[\"Main Category\"] ==k][\"Subcategory\"].unique()\n",
    "    for k,v in scraped_csv[scraped_csv[\"Main Category\"] ==k][\"Subcategory\"].value_counts().items():\n",
    "        if v<10:\n",
    "            print(f\"\\t{k} has only {v} images\")\n",
    "    id2subcat2id[i] = {k2: j for j, k2 in enumerate(subcats)}\n",
    "\n",
    "print(maincat2id, id2subcat2id)\n",
    "with open(\"maincat2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(maincat2id, f)\n",
    "with open(\"id2subcat2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2subcat2id, f)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "chicken 15\n",
      "\tchicken sausage has only 7 images\n",
      "beef 23\n",
      "\tprime rib has only 4 images\n",
      "turkey 20\n",
      "\tturkey salad has only 9 images\n",
      "\tturkey brine has only 6 images\n",
      "\tturkey meatloaf has only 6 images\n",
      "\tturkey appetizers has only 3 images\n",
      "vegetable 12\n",
      "seafood and fish 19\n",
      "pork 8\n",
      "\tpork sandwiches has only 8 images\n",
      "{'chicken': 0, 'beef': 1, 'turkey': 2, 'vegetable': 3, 'seafood and fish': 4, 'pork': 5} {0: {'chicken appetizers': 0, 'baked and roasted chicken': 1, 'chicken main dishes': 2, 'healthy chicken main dishes': 3, 'chicken wings': 4, 'chicken soup': 5, 'chicken stew': 6, 'chicken salad': 7, 'chicken stir-fry': 8, 'chicken pie': 9, 'chicken breasts': 10, 'slow cooker chicken main dishes': 11, 'gourmet chicken main dishes': 12, 'whole chicken': 13, 'chicken thighs': 14, 'bbq & grilled chicken': 15, 'chicken tenders': 16, 'fried chicken': 17, 'chicken sausage': 18, 'chicken legs': 19, 'chicken sandwiches': 20, 'chicken chili': 21, 'ground chicken': 22}, 1: {'beef soup': 0, 'beef chili': 1, 'beef stew': 2, 'beef casserole': 3, 'bbq & grilled beef': 4, 'beef appetizers': 5, 'beef pie': 6, 'beef steaks': 7, 'beef tenderloin': 8, 'beef sandwiches': 9, 'beef chuck': 10, 'beef short loin': 11, 'beef brisket': 12, 'beef ribs': 13, 'ground beef': 14, 'hamburgers': 15, 'beef meatloaf': 16, 'beef stir-fry': 17, 'veal recipes': 18, 'prime rib': 19}, 2: {'turkey sausage': 0, 'turkey main dishes': 1, 'turkey soup': 2, 'turkey casserole': 3, 'ground turkey': 4, 'turkey appetizers': 5, 'turkey salad': 6, 'turkey meatloaf': 7, 'turkey chili': 8, 'turkey brine': 9, 'whole turkey': 10, 'turkey breasts': 11}, 3: {'summer squash side dishes': 0, 'roasted vegetables': 1, 'asparagus side dishes': 2, 'tomato side dishes': 3, 'cauliflower side dishes': 4, 'cabbage side dishes': 5, 'squash side dishes': 6, 'broccoli side dishes': 7, 'carrot side dishes': 8, 'potato side dishes': 9, 'zucchini side dishes': 10, 'corn side dishes': 11, 'sweet potato side dishes': 12, 'onion side dishes': 13, 'eggplant side dishes': 14, 'greens side dishes': 15, 'green bean side dishes': 16, 'brussels sprouts side dishes': 17, 'green pea side dishes': 18}, 4: {'seafood main dishes': 0, 'gourmet seafood main dishes': 1, 'healthy seafood main dishes': 2, 'shellfish recipes': 3, 'fish recipes': 4, 'seafood pasta salad': 5, 'bbq & grilled seafood': 6, 'seafood stew': 7}, 5: {'pork ribs': 0, 'pork casserole': 1, 'pork chili': 2, 'pork shoulder': 3, 'bbq & grilled pork': 4, 'pork sandwiches': 5, 'pork tenderloin': 6, 'pork sausage': 7, 'pork soup': 8, 'pork stew': 9, 'pork pie': 10, 'pork stir-fry': 11, 'pork meatloaf': 12, 'ground pork': 13, 'ham recipes': 14}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('imagetorecipe_env': conda)"
  },
  "interpreter": {
   "hash": "28551ffe538f243c191629ed63097f17d7aaec81a78654141724bad4bbe4364f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}